# Лекция "Частотные списки и N-граммы"

## 1. Частотные списки

**Частотный список** — это структура данных, которая хранит информацию о том, сколько раз каждый элемент встречается в наборе данных (например, в тексте или коллекции слов). В контексте анализа текста, частотные списки часто используются для изучения частоты появления слов или фраз.

### 1. Пример создания частотного списка для текста

Для создания частотного списка сначала нужно разделить текст на отдельные слова (или токены). Затем можно посчитать, сколько раз каждое слово встречается в тексте.

*import re*

*from collections import Counter*

*# Пример текста*

*text = "Анализ данных помогает извлекать полезную информацию из данных."*

*# Токенизация текста: разбиваем на слова*

*words = re.findall(r'\w+', text.lower())  # \w+ находит все последовательности букв и цифр*

*# Создаем частотный список с помощью Counter*

*word\_freq = Counter(words)*

*# Выводим частотный список*

*print(word\_freq)*



**Пояснение:**
- Мы используем регулярные выражения (через re.findall) для извлечения слов из текста. Все слова приводятся к нижнему регистру с помощью text.lower(), чтобы избежать учета регистра при подсчете частоты.
- Counter из библиотеки collections автоматически создает частотный список, где ключи — это слова, а значения — количество их появлений в тексте.


Результат:

Counter({'данных': 2, 'анализ': 1, 'помогает': 1, 'извлекать': 1, 'полезную': 1, 'информацию': 1})

**Зачем использовать частотные списки?**

Частотные списки помогают:

- Оценивать значимость слов в тексте (например, при анализе частоты встречаемости определенных терминов).
- Определять ключевые слова в тексте.
- Выполнять фильтрацию слов (например, удалить стоп-слова).


## 2. N-граммы

**N-граммы** — это последовательности из n элементов (чаще всего слов или символов) в тексте. Например, биграммы — это пары последовательных слов, триграммы — тройки последовательных слов и так далее.

В контексте анализа текста n-граммы полезны для:

- Изучения контекста появления слов.
- Построения моделей языка, например, для машинного обучения.
- Анализа повторяющихся фраз и словосочетаний.

### 1. Пример создания биграмм

Давайте посмотрим, как создать биграммы для текста, т.е. пары последовательных слов.

*from nltk import ngrams*

*# Пример текста*

*text = "Анализ данных помогает извлекать полезную информацию из данных."*

*# Токенизация текста*

*words = re.findall(r'\w+', text.lower())*

*# Создаем биграммы (пары последовательных слов)*

*bigrams = list(ngrams(words, 2))*

*# Выводим биграммы*

*print(bigrams)*


**Пояснение:**

- Мы снова используем регулярные выражения для разбивки текста на слова.
- Функция ngrams из библиотеки nltk генерирует последовательности из n элементов. В данном случае мы передаем 2, чтобы создать биграммы.

Результат:

[('анализ', 'данных'), ('данных', 'помогает'), ('помогает', 'извлекать'), ('извлекать', 'полезную'), ('полезную', 'информацию'), ('информацию', 'из'), ('из', 'данных')]

### 2. Пример создания триграмм
Если мы хотим работать с триграммами (т.е. с тройками последовательных слов), мы можем изменить параметр n на 3:

*Создаем триграммы (тройки последовательных слов)*

*trigrams = list(ngrams(words, 3))*

*# Выводим триграммы*

*print(trigrams)*

Результат:

[('анализ', 'данных', 'помогает'), ('данных', 'помогает', 'извлекать'), ('помогает', 'извлекать', 'полезную'), ('извлекать', 'полезную', 'информацию'), ('полезную', 'информацию', 'из'), ('информацию', 'из', 'данных')]

**Зачем использовать n-граммы?**

N-граммы помогают:

- Захватывать локальные зависимости между словами (например, в машинном переводе, когда важен контекст).
- Оценивать вероятность появления определенных фраз или последовательностей в тексте.
- Строить модели для предсказания следующего слова на основе контекста.


## 3. Анализ частоты n-грамм
Теперь, когда мы познакомились с понятием n-грамм, давайте рассмотрим, как мы можем анализировать частотность различных n-грамм в тексте. Это может быть полезно для выявления наиболее популярных фраз, характерных для данного корпуса текста.

### 1. Пример подсчета частотности n-грамм
Мы можем использовать тот же текст и анализировать, сколько раз каждая биграмма или триграмма встречается. Для этого можно использовать Counter для подсчета количества повторений каждой n-граммы.

*from collections import Counter*

*# Пример текста*

*text = "Анализ данных помогает извлекать полезную информацию из данных."*

*# Токенизация текста*

*words = re.findall(r'\w+', text.lower())*

*# Генерация биграмм*

*bigrams = list(ngrams(words, 2))*

*# Подсчет частоты биграмм*

*bigram\_freq = Counter(bigrams)*

*# Выводим частотность биграмм*

*print(bigram\_freq)*

**Пояснение:**

- Мы генерируем биграммы, как и раньше, с помощью ngrams.
- Затем мы используем Counter для подсчета частоты каждой биграммы.
- Counter возвращает частотный список, где ключами будут являться биграммы, а значениями — количество их встреч.


Результат:

*Counter({('данных', 'помогает'): 1, ('анализ', 'данных'): 1, ('помогает', 'извлекать'): 1, ('извлекать', 'полезную'): 1, ('полезную', 'информацию'): 1, ('информацию', 'из'): 1, ('из', 'данных'): 1})*

### 2. Подсчет частоты триграмм
Точно так же мы можем подсчитать частоту триграмм:

**Генерация триграмм**

*trigrams = list(ngrams(words, 3))*

*# Подсчет частоты триграмм*

*trigram\_freq = Counter(trigrams)*

*# Выводим частотность триграмм*

*print(trigram\_freq)*

Результат:

*Counter({('анализ', 'данных', 'помогает'): 1, ('данных', 'помогает', 'извлекать'): 1, ('помогает', 'извлекать', 'полезную'): 1, ('извлекать', 'полезную', 'информацию'): 1, ('полезную', 'информацию', 'из'): 1, ('информацию', 'из', 'данных'): 1})*

**Зачем это нужно?**

Анализ частоты n-грамм помогает:

- Понимать, какие фразы или последовательности слов наиболее характерны для текста.
- Определять повторяющиеся шаблоны в данных.
- Использовать для построения моделей предсказания следующего слова или фразы в тексте.

В реальных задачах, например, при анализе больших объемов текста (новости, отзывы, статьи), мы можем использовать эти частоты для:

- Выявления ключевых фраз.
- Сравнения разных текстов по частотам появления схожих фраз.
- Построения лексических моделей, как в поисковых системах.

## 4. Фильтрация частотных n-грамм
Когда мы анализируем n-граммы, иногда нам нужно удалить из них стоп-слова (например, такие как "и", "в", "на", "по"), которые не несут значимой информации. Это особенно важно, когда мы хотим сосредоточиться на более значимых фразах или словах.

**Пример фильтрации n-грамм с использованием стоп-слов**
Для фильтрации n-грамм мы можем использовать список стоп-слов. В Python существует множество библиотек, например, nltk, которая предоставляет готовые списки стоп-слов для различных языков.

*from nltk.corpus import stopwords*

*from nltk import ngrams*

*import re*

*from collections import Counter*

*# Загрузим стоп-слова для русского языка*

*stop\_words = set(stopwords.words('russian'))*

*# Пример текста*

*text = "Анализ данных помогает извлекать полезную информацию из данных."*

*# Токенизация текста*

*words = re.findall(r'\w+', text.lower())*

*# Генерация биграмм*

*bigrams = list(ngrams(words, 2))*

*# Фильтруем биграммы, исключая те, где одно или оба слова — стоп-слова*

*filtered\_bigrams = [bigram for bigram in bigrams if bigram[0] not in stop\_words and bigram[1] not in stop\_words]*

*# Подсчитываем частоту оставшихся биграмм*

*bigram\_freq = Counter(filtered\_bigrams)*

*# Выводим частотность отфильтрованных биграмм*

*print(bigram\_freq)*

**Пояснение:**

- Мы используем библиотеку nltk.corpus.stopwords для получения списка стоп-слов.
- В процессе фильтрации мы исключаем те биграммы, где хотя бы одно из слов является стоп-словом.
- Затем считаем частотность оставшихся, значимых биграмм.

**Результат:**

*Counter({('анализ', 'данных'): 1, ('данных', 'помогает'): 1, ('помогает', 'извлекать'): 1, ('извлекать', 'полезную'): 1, ('полезную', 'информацию'): 1, ('информацию', 'данных'): 1})*

**Почему фильтрация важна?**

Фильтрация позволяет:

- Убрать из анализа несущественные слова, которые часто встречаются, но не несут ценности.
- Сосредоточиться на более информативных n-граммах, которые действительно могут быть полезны для анализа или построения моделей.
- Повысить точность анализа, особенно когда работаете с большими текстами или сложными задачами, например, в машинном обучении.

**Пример фильтрации на основе частоты**

Мы можем также использовать фильтрацию по частоте, исключая те n-граммы, которые встречаются слишком редко или слишком часто.

*# Фильтруем биграммы, которые встречаются менее 2 раз*

*filtered\_bigrams\_by\_freq = {bigram: count for bigram, count in bigram\_freq.items() if count >= 2}*

*# Выводим результат*

*print(filtered\_bigrams\_by\_freq)*

Результат:

{}

В этом примере мы оставляем только те биграммы, которые встречаются два и более раз. Однако в данном случае таких нет, так как каждая биграмма встретилась только один раз.





































